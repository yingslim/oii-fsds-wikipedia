{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Social Data Science \n",
    "## Week 2 Day 2 Lab: Downloading to Wikipedia\n",
    "\n",
    "Today we will review some changes to the Wikipedia code. These changes will considerably alter what you are able to do with this code. The end result will be a set of two folders, `data` and `dataframes` which you can use for analysis of Wikipedia. \n",
    "\n",
    "The code has now been altered on my end in several ways: \n",
    "- use and report curl from special export to get a complete history of a page. \n",
    "- considerably expanded reporting and commenting.\n",
    "- new arguments available to the script include --count_only \n",
    "\n",
    "There is also now a second script available `xml_to_dataframe.py` which can be used to then process these files and turn them into separate DataFrames. These DataFrames are stored as .feather files and can be loaded with the code below. \n",
    "\n",
    "You should review the `xml_to_dataframe.py` file as all the operations within that file have been covered in class with the exception of TQDM but you can see how that works in practice. \n",
    "\n",
    "You will note that this version does not use recursion to count the files. Instead it more literally looks within year and month. This is sufficient for this work, but with a deeper folder structure and one where the structure is less certain this approach would not be robust. On the other hand, by assuming year and month it allows for some interesting statistics about the year and month to be displayed. In your own work you may now consider whether to approach a task with a more general but often more abstract solution or a more specific but often more fragile solution. You can see in Jon's solution that he used a clever way to simply count all the files using a global and letting the global handle the recursion (`download_and_count_revisions_solution.py`).\n",
    "\n",
    "You should now be able to download a complete history for a single wikipedia page and process that as a DataFrame. Confirm that you can do this with the code yourself. Then discuss among your group:\n",
    "1. Which two (or more) public figures are worth comparing and why. \n",
    "2. Prior to any specific time series analysis, consider your expectations for this exploratory comparison.  \n",
    "\n",
    "Draw upon your group's potential expertise in social science to come up with a theoretically informed rationale for a given comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging in Changes to a Repository \n",
    "\n",
    "First you will want to merge files from an upstream branch (mine). These instructions will show how to do that from the terminal. You will want to be in the oii-fsds-wikipedia folder when entering these commands. Note especially **Step 3**. If you do this it will overwrite `download_wiki_revisions.py` so consider making a backup. \n",
    "\n",
    "1. **Add the original repository as a remote:**\n",
    "   ```sh\n",
    "   git remote add upstream https://github.com/berniehogan/oii-fsds-wikipedia.git\n",
    "   ```\n",
    "\n",
    "2. **Fetch the changes from the original repository:**\n",
    "   ```sh\n",
    "   git fetch upstream\n",
    "   ```\n",
    "\n",
    "3. **Backup any local changes:**\n",
    "   If you have your own versions of files like `download_wiki_revisions.py`, you should rename the file first to avoid conflicts:\n",
    "   ```sh\n",
    "   mv download_wiki_revisions.py download_wiki_revisions_backup.py\n",
    "   ```\n",
    "\n",
    "4. **Merge upstream changes into your local main branch:**\n",
    "   ```sh\n",
    "   git merge upstream/main\n",
    "   ```\n",
    "\n",
    "5. **Resolve any conflicts and commit the changes:**\n",
    "   You should resolve any conflicts that arise during the merge and then commit the changes:\n",
    "   ```sh\n",
    "   git add .\n",
    "   git commit -m \"Merge changes from upstream\"\n",
    "   ```\n",
    "\n",
    "6. **Push the changes to your GitHub repository:**\n",
    "   ```sh\n",
    "   git push origin main\n",
    "   ```\n",
    "\n",
    "7. **Test your code after merging:**\n",
    "   You should test your code to ensure everything works correctly after the integration.\n",
    "\n",
    "By following these steps, you should be able to integrate the latest changes from my repository while preserving your own custom modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-5.3.0-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n",
      "Downloading lxml-5.3.0-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.0/3.8 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 8.4 MB/s eta 0:00:00\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-5.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, you can use the script below if you wish in order to run the commands directly within a Jupyter notebook rather than via that terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\limyi\\\\OneDrive\\\\Documents\\\\oxford\\\\Fundamentals for Social Data Science\\\\Week2\\\\oii-fsds-wikipedia'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading revisions for first article...\n",
      "\n",
      "Downloading revisions for second article...\n",
      "Finish downloding\n",
      "\n",
      "Converting revisions to DataFrames...\n",
      "\n",
      "Verifying DataFrame contents...\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 648 entries, 647 to 0\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype              \n",
      "---  ------       --------------  -----              \n",
      " 0   revision_id  648 non-null    object             \n",
      " 1   timestamp    648 non-null    datetime64[ns, UTC]\n",
      " 2   username     466 non-null    object             \n",
      " 3   userid       466 non-null    object             \n",
      " 4   comment      527 non-null    object             \n",
      " 5   text_length  648 non-null    int64              \n",
      " 6   year         648 non-null    object             \n",
      " 7   month        648 non-null    object             \n",
      "dtypes: datetime64[ns, UTC](1), int64(1), object(6)\n",
      "memory usage: 45.6+ KB\n",
      "None\n",
      "\n",
      "First few rows:\n",
      "    revision_id                 timestamp      username    userid  \\\n",
      "647  1244076203 2024-09-04 22:32:11+00:00    Arachnidly  47739713   \n",
      "646  1244075613 2024-09-04 22:27:46+00:00    Arachnidly  47739713   \n",
      "645  1243594551 2024-09-02 10:24:08+00:00  Michaelmalak  14994222   \n",
      "644  1243591540 2024-09-02 09:49:07+00:00  Michaelmalak  14994222   \n",
      "643  1243117674 2024-08-30 16:15:06+00:00          None      None   \n",
      "\n",
      "                                               comment  text_length  year  \\\n",
      "647  removed cuz scientific method linked in intro ...        28706  2024   \n",
      "646  removed unrelated journal \"scientific data\" pa...        28730  2024   \n",
      "645  Undid revision [[Special:Diff/1243592758|12435...        28754  2024   \n",
      "644  Undid revision [[Special:Diff/1243589808|12435...        28754  2024   \n",
      "643                                               None        28754  2024   \n",
      "\n",
      "    month  \n",
      "647    09  \n",
      "646    09  \n",
      "645    09  \n",
      "644    09  \n",
      "643    08  \n",
      "\n",
      "Basic statistics:\n",
      "Total number of revisions: 648\n",
      "Date range: from 2012-04-11 17:36:25+00:00 to 2024-09-04 22:32:11+00:00\n",
      "Number of unique editors: 232\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define articles we want to download\n",
    "article1 = \"Data_science\"\n",
    "article2 = \"Machine_learning\"\n",
    "\n",
    "# Create necessary directories if they don't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"DataFrames\", exist_ok=True)\n",
    "\n",
    "# Download revisions for both articles\n",
    "print(\"Downloading revisions for first article...\")\n",
    "os.system(f'python ./download_wiki_revisions.py \"{article1}\"')\n",
    "print(\"\\nDownloading revisions for second article...\")\n",
    "os.system(f'python ./download_wiki_revisions.py \"{article2}\"')\n",
    "\n",
    "print('Finish downloding')\n",
    "# Convert all downloaded revisions to DataFrames\n",
    "print(\"\\nConverting revisions to DataFrames...\")\n",
    "os.system('python xml_to_dataframe.py --data-dir ./data --output-dir ./DataFrames')\n",
    "\n",
    "# Load and verify one of the DataFrames\n",
    "print(\"\\nVerifying DataFrame contents...\")\n",
    "df = pd.read_feather(f\"DataFrames/{article1}.feather\")\n",
    "\n",
    "# Display basic information about the DataFrame\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display some basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(f\"Total number of revisions: {len(df)}\")\n",
    "print(f\"Date range: from {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"Number of unique editors: {df['username'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
