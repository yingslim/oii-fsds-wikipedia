{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Social Data Science \n",
    "## Week 2 Day 2 Lab: Downloading to Wikipedia\n",
    "\n",
    "Today we will review some changes to the Wikipedia code. These changes will considerably alter what you are able to do with this code. The end result will be a set of two folders, `data` and `dataframes` which you can use for analysis of Wikipedia. \n",
    "\n",
    "The code has now been altered on my end in several ways: \n",
    "- use and report curl from special export to get a complete history of a page. \n",
    "- considerably expanded reporting and commenting.\n",
    "- new arguments available to the script include --count_only \n",
    "\n",
    "There is also now a second script available `xml_to_dataframe.py` which can be used to then process these files and turn them into separate DataFrames. These DataFrames are stored as .feather files and can be loaded with the code below. \n",
    "\n",
    "You should review the `xml_to_dataframe.py` file as all the operations within that file have been covered in class with the exception of TQDM but you can see how that works in practice. \n",
    "\n",
    "You will note that this version does not use recursion to count the files. Instead it more literally looks within year and month. This is sufficient for this work, but with a deeper folder structure and one where the structure is less certain this approach would not be robust. On the other hand, by assuming year and month it allows for some interesting statistics about the year and month to be displayed. In your own work you may now consider whether to approach a task with a more general but often more abstract solution or a more specific but often more fragile solution. You can see in Jon's solution that he used a clever way to simply count all the files using a global and letting the global handle the recursion (`download_and_count_revisions_solution.py`).\n",
    "\n",
    "You should now be able to download a complete history for a single wikipedia page and process that as a DataFrame. Confirm that you can do this with the code yourself. Then discuss among your group:\n",
    "1. Which two (or more) public figures are worth comparing and why. \n",
    "2. Prior to any specific time series analysis, consider your expectations for this exploratory comparison.  \n",
    "\n",
    "Draw upon your group's potential expertise in social science to come up with a theoretically informed rationale for a given comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging in Changes to a Repository \n",
    "\n",
    "First you will want to merge files from an upstream branch (mine). These instructions will show how to do that from the terminal. You will want to be in the oii-fsds-wikipedia folder when entering these commands. Note especially **Step 3**. If you do this it will overwrite `download_wiki_revisions.py` so consider making a backup. \n",
    "\n",
    "1. **Add the original repository as a remote:**\n",
    "   ```sh\n",
    "   git remote add upstream https://github.com/berniehogan/oii-fsds-wikipedia.git\n",
    "   ```\n",
    "\n",
    "2. **Fetch the changes from the original repository:**\n",
    "   ```sh\n",
    "   git fetch upstream\n",
    "   ```\n",
    "\n",
    "3. **Backup any local changes:**\n",
    "   If you have your own versions of files like `download_wiki_revisions.py`, you should rename the file first to avoid conflicts:\n",
    "   ```sh\n",
    "   mv download_wiki_revisions.py download_wiki_revisions_backup.py\n",
    "   ```\n",
    "\n",
    "4. **Merge upstream changes into your local main branch:**\n",
    "   ```sh\n",
    "   git merge upstream/main\n",
    "   ```\n",
    "\n",
    "5. **Resolve any conflicts and commit the changes:**\n",
    "   You should resolve any conflicts that arise during the merge and then commit the changes:\n",
    "   ```sh\n",
    "   git add .\n",
    "   git commit -m \"Merge changes from upstream\"\n",
    "   ```\n",
    "\n",
    "6. **Push the changes to your GitHub repository:**\n",
    "   ```sh\n",
    "   git push origin main\n",
    "   ```\n",
    "\n",
    "7. **Test your code after merging:**\n",
    "   You should test your code to ensure everything works correctly after the integration.\n",
    "\n",
    "By following these steps, you should be able to integrate the latest changes from my repository while preserving your own custom modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, you can use the script below if you wish in order to run the commands directly within a Jupyter notebook rather than via that terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define articles we want to download\n",
    "article1 = \"Data_science\"\n",
    "article2 = \"Machine_learning\"\n",
    "\n",
    "# Create necessary directories if they don't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"DataFrames\", exist_ok=True)\n",
    "\n",
    "# Download revisions for both articles\n",
    "print(\"Downloading revisions for first article...\")\n",
    "os.system(f'python download_wiki_revisions.py \"{article1}\"')\n",
    "print(\"\\nDownloading revisions for second article...\")\n",
    "os.system(f'python download_wiki_revisions.py \"{article2}\"')\n",
    "\n",
    "# Convert all downloaded revisions to DataFrames\n",
    "print(\"\\nConverting revisions to DataFrames...\")\n",
    "os.system('python xml_to_dataframe.py --data-dir ./data --output-dir ./DataFrames')\n",
    "\n",
    "# Load and verify one of the DataFrames\n",
    "print(\"\\nVerifying DataFrame contents...\")\n",
    "df = pd.read_feather(f\"DataFrames/{article1}.feather\")\n",
    "\n",
    "# Display basic information about the DataFrame\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display some basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(f\"Total number of revisions: {len(df)}\")\n",
    "print(f\"Date range: from {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"Number of unique editors: {df['username'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
